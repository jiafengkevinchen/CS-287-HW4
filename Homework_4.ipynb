{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OSMDXPy8M7C"
   },
   "source": [
    "# HW 4 - All About Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zz5Kh9F0xBVf"
   },
   "source": [
    "Welcome to CS 287 HW4. To begin this assignment first turn on the Python 3 and GPU backend for this Colab by clicking `Runtime > Change Runtime Type` above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MiugnUMt8M7E"
   },
   "source": [
    "In this homework you will be reproducing the decomposable attention model in Parikh et al. https://aclweb.org/anthology/D16-1244. (This is one of the models that inspired development of the transformer). \n",
    "\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "We ask that you finish the following goals in PyTorch:\n",
    "\n",
    "1. Implement the vanilla decomposable attention model as described in that paper.\n",
    "2. Implement the decomposable attention model with intra attention or another extension.\n",
    "3. Visualize the attentions in the above two parts.\n",
    "4. Implement a mixture of models with uniform prior and perform training with exact log marginal likelihood (see below for detailed instructions)\n",
    "5. Train the mixture of models in part 4 with VAE. (This may not produce a better model, this is still a research area) \n",
    "6. Interpret which component specializes at which type of tasks using the posterior.\n",
    "\n",
    "Consult the paper for model architecture and hyperparameters, but you are also allowed to tune the hyperparameters yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iG0DhOyL8M7E"
   },
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. You may construct your models inline or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-iEe85198M7F",
    "outputId": "6e6f60f3-3088-46f2-e30e-32a5a0e0240b"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchtext opt_einsum git+https://github.com/harvardnlp/namedtensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eHTkeBl-8M7I"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Text text processing library and methods for pretrained word embeddings\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "# Named Tensor wrappers\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KE37lf0u8M7L"
   },
   "source": [
    "The dataset we will use of this problem is known as the Stanford Natural Language Inference (SNLI) Corpus ( https://nlp.stanford.edu/projects/snli/ ). It is collection of 570k English sentence pairs with relationships entailment, contradiction, or neutral, supporting the task of natural language inference (NLI). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXlrvClg8M7M"
   },
   "source": [
    "To start, `torchtext` requires that we define a mapping from the raw text data to featurized indices. These fields make it easy to map back and forth between readable data and math, which helps for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbZiWCz18M7M"
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = NamedField(names=('seqlen',))\n",
    "\n",
    "# Our labels $y$\n",
    "LABEL = NamedField(sequential=False, names=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYweUw-h8M7Q"
   },
   "source": [
    "Next we input our data. Here we will use the standard SNLI train split, and tell it the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQcxFoh88M7R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading snli_1.0.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "snli_1.0.zip: 100%|██████████| 94.6M/94.6M [00:25<00:00, 3.73MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n"
     ]
    }
   ],
   "source": [
    "train, val, test = torchtext.datasets.SNLI.splits(\n",
    "    TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJOFyfYT8M7V"
   },
   "source": [
    "Let's look at this data. It's still in its original form, we can see that each example consists of a premise, a hypothesis and a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "vDgwcxmh8M7W",
    "outputId": "816b3dc6-2dde-4dce-f8eb-5dab9b2a14b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 549367\n",
      "vars(train[0]) {'premise': ['A', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane.'], 'hypothesis': ['A', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition.'], 'label': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Tcw37Xr8M7b"
   },
   "source": [
    "In order to map this data to features, we need to assign an index to each word an label. The function build vocab allows us to do this and provides useful options that we will need in future assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "otfqiXh98M7b",
    "outputId": "750a1094-1260-415a-baba-afa03f97f63f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 62998\n",
      "LABEL.vocab <torchtext.vocab.Vocab object at 0x104429588>\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('LABEL.vocab', LABEL.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dBj-iHi8M7f"
   },
   "source": [
    "Finally we are ready to create batches of our training data that can be used for training and validating the model. This function produces 3 iterators that will let us go through the train, val and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FV-CSDuX8M7g"
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=16, device=device, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpXAKSbA8M7i"
   },
   "source": [
    "Let's look at a single batch from one of these iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "colab_type": "code",
    "id": "dcsZVvaG8M7j",
    "outputId": "5e01b79e-c0d9-43e1-c5ff-032e049578a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of premise batch: OrderedDict([('seqlen', 22), ('batch', 16)])\n",
      "Size of hypothesis batch: OrderedDict([('seqlen', 17), ('batch', 16)])\n",
      "Second premise in batch NamedTensor(\n",
      "\ttensor([  3,  37,  11,  18,  10, 251,   4,   6, 226, 190,  24, 741,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1]),\n",
      "\t('seqlen',))\n",
      "Converted back to string: A group of people are laying in the grass under an umbrella. <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Second hypothesis in batch NamedTensor(\n",
      "\ttensor([ 52, 251,   4,   6, 226,   4,   6, 748,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1]),\n",
      "\t('seqlen',))\n",
      "Converted back to string: People laying in the grass in the rain. <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(\"Size of premise batch:\", batch.premise.shape)\n",
    "print(\"Size of hypothesis batch:\", batch.hypothesis.shape)\n",
    "premise = batch.premise.get(\"batch\", 1)\n",
    "print(\"Second premise in batch\", premise)\n",
    "print(\"Converted back to string:\", \" \".join([TEXT.vocab.itos[i] for i in premise.tolist()]))\n",
    "hypothesis = batch.hypothesis.get(\"batch\", 1)\n",
    "print(\"Second hypothesis in batch\", hypothesis)\n",
    "print(\"Converted back to string:\", \" \".join([TEXT.vocab.itos[i] for i in hypothesis.tolist()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJxDYWUp8M7m"
   },
   "source": [
    "Similarly it produces a vector for each of the labels in the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "k_DcwMRh8M7m",
    "outputId": "80f746d7-091a-4e98-a976-90cd0e6ac962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of label batch: OrderedDict([('batch', 10)])\n",
      "Second in batch 3\n",
      "Converted back to string: neutral\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of label batch:\", batch.label.shape)\n",
    "example = batch.label.get(\"batch\", 1)\n",
    "print(\"Second in batch\", example.item())\n",
    "print(\"Converted back to string:\", LABEL.vocab.itos[example.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgbiW5PP8M7r"
   },
   "source": [
    "Finally the Vocab object can be used to map pretrained word vectors to the indices in the vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "4ZVfhI6x8M7s",
    "outputId": "4d682ca2-7525-4e63-fae9-ab19b75f89a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings shape: OrderedDict([('word', 62998), ('embedding', 300)])\n",
      "Word embedding of 'follows', first 10 dim  NamedTensor(\n",
      "\ttensor([-0.0452, -0.0213,  0.0814,  0.0006, -0.0474,  0.0151, -0.0625, -0.0058,\n",
      "         0.0476, -0.1896]),\n",
      "\t('embedding',))\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary with word embeddings\n",
    "# Out-of-vocabulary (OOV) words are hashed to one of 100 random embeddings each\n",
    "# initialized to mean 0 and standarad deviation 1 (Sec 5.1)\n",
    "import random\n",
    "unk_vectors = [torch.randn(300) for _ in range(100)]\n",
    "TEXT.vocab.load_vectors(vectors='glove.6B.300d',\n",
    "                        unk_init=lambda x:random.choice(unk_vectors))\n",
    "# normalized to have l_2 norm of 1\n",
    "vectors = TEXT.vocab.vectors\n",
    "vectors = vectors / vectors.norm(dim=1,keepdim=True)\n",
    "vectors = NamedTensor(vectors, ('word', 'embedding'))\n",
    "TEXT.vocab.vectors = vectors\n",
    "print(\"Word embeddings shape:\", TEXT.vocab.vectors.shape)\n",
    "print(\"Word embedding of 'follows', first 10 dim \",\n",
    "      TEXT.vocab.vectors.get('word', TEXT.vocab.stoi['follows']) \\\n",
    "                        .narrow('embedding', 0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4s05JN38M71"
   },
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to implement the models described at the top of the assignment using the data given by this iterator. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWjR3JkUk1_A"
   },
   "source": [
    "### Instructions for latent variable mixture model.\n",
    "\n",
    "For the last part of this assignment we will consider a latent variable version of this model. This is a use of latent variable as a form of ensembling.\n",
    "\n",
    "Instead of a single model, we use $K$ models $p(y | \\mathbf{a}, \\mathbf{b}; \\theta_k)$ ($k=1,\\cdots,K$), where $K$ is a hyperparameter. Let's introduce a discrete latent variable $c\\sim \\text{Uniform}(1,\\cdots, K)$ denoting which model is being used to produce the label $y$, then the marginal likelihood is\n",
    "\n",
    "\n",
    "$$\n",
    "p(y|\\mathbf{a}, \\mathbf{b}; \\theta) = \\sum_{c=1}^K p(c) p(y | \\mathbf{a}, \\mathbf{b}; \\theta_c)\n",
    "$$\n",
    "\n",
    "When $K$ is small, we can *enumerate* all possible values of $c$ to maximize the log marginal likelihood. \n",
    "\n",
    "We can also use variational auto encoding to perform efficient training. We first introduce an inference network $q(c| y, \\mathbf{a}, \\mathbf{b})$, and the ELBO is\n",
    "\n",
    "$$\n",
    "\\log p(y|\\mathbf{a}, \\mathbf{b}; \\theta)  \\ge \\mathbb{E}_{c \\sim q(c|y, \\mathbf{a}, \\mathbf{b})} \\log p(y|\\mathbf{a},\\mathbf{b}; \\theta_c) - KL(q(c|y, \\mathbf{a}, \\mathbf{b})|| p(c)),\n",
    "$$\n",
    "\n",
    "where $p(c)$ is the prior uniform distribution. We can calculate the $KL$ term in closed form, but for the first term in ELBO, due to the discreteness of $c$, we cannot use the reparameterization trick. Instead we use REINFORCE to estimate the gradients (or see slides):\n",
    "\n",
    "$$\n",
    "\\nabla \\mathbb{E}_{c \\sim q(c|y, \\mathbf{a}, \\mathbf{b})} \\log p(y|\\mathbf{a},\\mathbf{b}; \\theta_c) = \\mathbb{E}_{c \\sim q(c|y, \\mathbf{a}, \\mathbf{b})} \\left [\\nabla \\log p(y|\\mathbf{a},\\mathbf{b}; \\theta_c) + \\log p(y|\\mathbf{a},\\mathbf{b}; \\theta_c)  \\nabla \\log q(c|y, \\mathbf{a}, \\mathbf{b})\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "At inference time, to get $p(y|\\mathbf{a}, \\mathbf{b}; \\theta)$ we use enumeration to calculate it exactly. For posterior inference, we can either use $q(c| y, \\mathbf{a}, \\mathbf{b})$ to approximate the true posterior or use Bayes rule to calculate the posterior exactly.\n",
    "\n",
    "To interpret what specialized knowledge each component $c$ learns, we can find those examples whose posterior reaches maximum at $c$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ptva0JXkxcF"
   },
   "source": [
    "When a model is trained, use the following test function to produce predictions, and then upload your best result to the kaggle competition:  https://www.kaggle.com/c/harvard-cs287-s19-hw4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kw_PRRx18M72"
   },
   "outputs": [],
   "source": [
    "def test_code(model):\n",
    "    \"All models should be able to be run with following command.\"\n",
    "    upload = []\n",
    "    # Update: for kaggle the bucket iterator needs to have batch_size 10\n",
    "    test_iter = torchtext.data.BucketIterator(test, train=False, batch_size=10)\n",
    "    for batch in test_iter:\n",
    "        # Your prediction data here (don't cheat!)\n",
    "        probs = model(batch.text)\n",
    "        # here we assume that the name for dimension classes is `classes`\n",
    "        _, argmax = probs.max('classes')\n",
    "        upload += argmax.tolist()\n",
    "\n",
    "    with open(\"predictions.txt\", \"w\") as f:\n",
    "        for u in upload:\n",
    "            f.write(str(u) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YiFYDx_58M76"
   },
   "source": [
    "In addition, you should put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/nlp-template"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework 4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
