\documentclass[12pt]{article}

% for footnotes
\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother

\usepackage{common}
\usepackage{macros}
\usepackage{nameref}
\usepackage{pdflscape}
\newcommand{\sts}{Seq2Seq}
\newcommand{\embed}{\mathsf{embed}}
\renewcommand{\trans}{^\intercal}
\renewcommand{\a}{\mathbf{a}}
\renewcommand{\b}{\mathbf{b}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\y}{\mathbf{y}}
\title{HW4: All about Attention}
\author{Jiafeng Chen \and Yufeng Ling \and
Francisco Rivera}

\begin{document}

\maketitle

\section{Introduction}
In this writeup we consider natural language inference--given a premise and a hypothesis, can we determine the entailment and contradiction relationship between them. The key to the model is the attention architecture, which serves to decompose the problem into aligned subphrases. Not only does this design make training parellelizable, it also significantly reduces the number of parameters while delivering state-of-the-art results.

\section{Problem Description}
In this writeup, we consider the problem of natural language inference. Let 
\begin{equation}
	\bm a = (a_1, \dots, a_{\ell_a})
\end{equation}
be the premise of length $\ell_a$ and let
\begin{equation}
	\bm b = (b_1, \dots, b_{\ell_b})
\end{equation}
be the hypothesis of length $\ell_b$. Each $a_i, b_j \in \R^d$ is a word embedding vector of dimension $d$. Our goal is to, given the input pair $\bm a, \bm b$, predict the relationship $y \in \{y_1, \dots, y_C\}$ where $C$ is the number of output classes.

\section{Model and Algorithms}

\subsection{Decomposable Attention Model}
\label{sub:decomp_attn}
This section follows the architecture of \cite{parikh2016decomposable}.

\subsubsection{Vanilla model}
\label{ssub:vanilla_model}
We first look at the most basic approach that is the foundation of this architecture. We start by setting the inputs $\bar \a, \bar \b$ to be the premise and hypothesis $\a, \b$ themselves. We generate attention weight matrix by softmaxing over
\begin{equation}
	e_{ij} := F'(\bar a_i, \bar b_j) = F(\bar a_i)\trans F(\bar b_j).
\end{equation}
Note that we made the simplification of setting $F'$ to be the dot product of $\bar a_i$ and $\bar b_j$ through the same feed-forward neural network, which reduces the number of operations from $O(\ell_a \times \ell_b)$ to $O(\ell_a + \ell_b)$. The attended phrases are then
\begin{align}
	\beta_i &:= \sum_{j=1}^{\ell_b} \frac{\exp(e_{ij})}{\sum_{k=1}^{\ell_b} \exp(e_{ik})} \bar b_j, \nonumber\\
	\alpha_j &:= \sum_{i=1}^{\ell_b} \frac{\exp(e_{ij})}{\sum_{k=1}^{\ell_a} \exp(e_{kj})} \bar a_i.
\end{align}
A key implementation detail is that we need to apply masking to the $\{e_{ij}\}$ matrix before softmaxing to avoid putting attention on padding.

Next, we compare the aligned attended phrases to the original ones by concatenating them and applying a feed-forward neural network $G$.
\begin{align}
	\v_{1,i} &:= G([\bar a_i, \beta_i]) \quad \forall i \in [1, \dots, \ell_a], \nonumber\\
	\v_{2,j} &:= G([\bar b_j, \alpha_j]) \quad \forall j \in [1, \dots, \ell_b].
\end{align}
We then apply sum-over-time pooling, with padding masked out, to generate the penultimate vectors
\begin{equation}
	\v_1 = \sum_{i=1}^{\ell_a} \v_{1, i}, \qquad \v_2 = \sum_{j=1}^{\ell_b} \v_{2, j}.
\end{equation}
Finally, we apply a feed-forward neural network $H$ to the concatenated vectors to generate the unnormalized predictions for each class
\begin{equation}
	\hat y = H([\v_1, \v_2]) \in \R^C.
\end{equation}
The prediction is $\hat y = \arg\max_i \hat \y_i$. In the training of this model, we use the multi-class cross-entrophy loss as the loss function.
\begin{equation}
	L(\theta_F, \theta_G, \theta_H) = \frac1N \sum_{n=1}^N \sum_{c=1}^C y_c^{(n)} \log \frac{\exp(\hat y_c)}{\sum_{c'=1}^C \exp(\hat y_{c'})}.
\end{equation}

% subsubsection vanilla_model (end)

\subsubsection{Intra-Sentence Attention} % (fold)
\label{ssub:intra_sentence_attention}
We can improve the model by incorporating intra-sentence attention. Instead of having $(\bar \a, \bar \b) = (\a, \b)$, we add self-attention to the input. We let the unnormalized attention weights be
\begin{equation}
	f_{ij} = F_\mathrm{intra}(a_i)\trans F_\mathrm{intra}(a_j),
\end{equation}
where $F_\mathrm{intra}$ is a feed-forward network. We then create the self-aligned phrases
\begin{equation}
	a_i' = \sum_{j=1}^{\ell_a} \frac{\exp(f_{ij} + d_{i-j})}{\sum_{k=1}^{\ell_a} \exp(f_{ik} + d_{i-k})} a_j.
\end{equation}
In the above equation, $d_{i-j} \in \R$ is the bias term based on distance, which is shared throughout sentences. Moreover, we bucket the terms such that all distances greater than 10 have the same bias. In the end, we use $\bar a_i = [a_i, a_i']$ and $\bar b_j = [b_j, b_j']$ as inputs.

% subsubsection intra_sentence_attention (end)

\subsection{Latent Variable Mixture Model} % (fold)
\label{sub:latent_variable_mixture_model}

% subsection latent_variable_mixture_model (end)

\section{Experiments}

\begin{landscape}
\begin{table}[tb]
    
    \centering
\begin{tabular}{ll}
\toprule
model name                   &specifications\\
\midrule
\texttt{Vanilla} & 100 embedding, 100 hidden, no dropout\\
\texttt{Intra-Attn} & Bi-directional LSTM, 100 embedding, 100 hidden, no dropout\\
\bottomrule
\end{tabular}
    \caption{Both models used only one layer in the decoder and trained with Adam and learning rate $10^{-3}$ over 10 epochs. For \texttt{Seq2Seq}, decrease in validation loss flattened at epoch 8. For \texttt{Seq2SeqAttn}, training stopped at epoch 7 after loss starting going up.}
    \label{tab:spec}
\end{table}
\begin{table}[h]
\centering
\begin{tabular}{lrrr}
\toprule
{}                                     & Validation loss & Validation accuracy \\
model name                             &       &        & \\
\midrule
\texttt{Vanilla}                      & 0.612  & 74.44\% \\
\texttt{Intra-Attn}  & 0.745   & 62.06\% \\
\texttt{Exact-Ensemble}  & 0.650   & 71.65\% \\
\bottomrule
\end{tabular}
\caption{Performance metrics for different models}
\label{table:performance}
\end{table}
\end{landscape}

\bibliographystyle{apalike}
\bibliography{writeup}

\appendix
\section{Model implementation}
% \lstinputlisting[caption=Sequence-to-Sequence]{models/seq2seq.py}

% This is a test for merging
% QED





\end{document}
